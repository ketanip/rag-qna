# RAG QnA ğŸ¤–ğŸ“Š

In this project we use a Vector Database (Chroma DB) backed LLM Inference using LangChain to create a QnA system.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ketanip/VectorDB-LangChain-LLM)

*While running in `colab` use real OpenAI credentials for LLM inference.*

# Running it 

```bash
git clone https://github.com/ketanip/VectorDB-LangChain-LLM.git
```

Open `src/code.ipynb` and execute cells one by one.

# ğŸ› ï¸ Tools used in this project:
1. ğŸ”— LangChain 
2. ğŸ“Š Vector Database ([Chroma DB](https://www.trychroma.com/)) 
3. ğŸ’¡ LLM (Zephyr 7B Î² - Mistral Architecture) powered by [LM Studio](https://lmstudio.ai/). 

# ğŸš€ Features:
1. Loads data from the `raw_data` folder ğŸ“‚.
2. Generates vector embeddings for the data and stores it in Chroma DB.
3. Searches relevant data based on the query ğŸ”.
4. Combines that data with the input question in a custom prompt template using LangChain.
5. Infers it using LLM and displays the results ğŸ¤–.
